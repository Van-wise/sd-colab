{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQjrBnpQrdnZ"
      },
      "source": [
        "###  <a href='https://ko-fi.com/P5P1QO6VB' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi4.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC9eV75zqF82"
      },
      "source": [
        "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
        "    <img src=\"https://i.ibb.co/w6ngBsW/20231031152934.jpg\" alt=\"20231031152934\" width=\"300\">\n",
        "    <span style=\"margin: 0 20px;\">|</span>\n",
        "    <img src=\"https://i.ibb.co/SrxYKmz/20231031152928.jpg\" alt=\"20231031152928\" width=\"400\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjNAgH2r9Fe"
      },
      "source": [
        "## InvokeAI_colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwwzZ0Mm-ri1"
      },
      "source": [
        "---\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Van-wise/sd-colab/blob/main/Invoke/IAekovnI_colab.ipynb)\n",
        "[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FVan-wise%2Fsd-colab%2Fblob%2Fmain%2FInvoke%2FIAekovnI_colab.ipynb&label=Page%20View%3A&labelColor=%232ccce4&countColor=%23555555&style=flat)](https://visitorbadge.io/status?path=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FVan-wise%2Fsd-colab%2Fblob%2Fmain%2FInvoke%2FIAekovnI_colab.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<font color=\"red\">**更新地址**：https://github.com/Van-wise/sd-colab\n",
        "\n",
        "**如果遇到报错,请联系up：**\n",
        "[bilibili](https://space.bilibili.com/1308057/channel/collectiondetail?sid=1365244) or [issues](https://github.com/Van-wise/sd-colab/issues)\n",
        "\n",
        "<font color=\"black\">**目前BUG：使用SDXL模型会断开连接.. 原因可能是colab进行了限制,sdxl模型使用RAM过高.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-60D2Oa2r9IJ"
      },
      "outputs": [],
      "source": [
        "# @title (🔆)安装环境\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from IPython.utils import capture\n",
        "from IPython.display import clear_output, display, HTML\n",
        "import multiprocessing\n",
        "\n",
        "# 忽略警告\n",
        "import warnings\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "!git config --global advice.detachedHead false\n",
        "!sed -i 's@text = _formatwarnmsg(msg)@text =\\\"\\\"@g' {warnings.__file__}\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# @markdown ---\n",
        "#@markdown ###<font color=\"#11659a\"> 选择分支：</font>\n",
        "#@markdown #### v3.5.1之后的版本UI界面有所改变。 v3.4.0 部署时间快,只需3分钟,其他大概8-10分钟。\n",
        "branchname = \"v3.4.0post2\" # @param [\"v3.6.2\", \"v3.5.1\", \"v3.4.0post2\", \"v3.3.0post3\", \"main\"]\n",
        "\n",
        "#@markdown #####<font color=\"#2c9678\">☑检查是否使用了GPU；☐不检查。\n",
        "_check_gpu = True  #@param {type:\"boolean\"}\n",
        "\n",
        "def check_gpu():\n",
        "    import tensorflow as tf\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    if len(physical_devices) > 0:\n",
        "        print(\"\\033[96m GPU is available.\\033[0m\", flush=True)\n",
        "    else:\n",
        "        print(\"\\033[91m 没有使用GPU,请在代码执行程序-更改运行时类型-设置为GPU!\\033[0m\", flush=True)\n",
        "        display(HTML(\"<img src='https://i.ibb.co/xfb7pB7/check-gpu.png' width='560px'/>\"))\n",
        "        from google.colab import runtime\n",
        "        runtime.unassign()\n",
        "\n",
        "airoot_dir = os.environ['AIROOT_DIR'] = os.getcwd() if 'AIROOT_DIR' not in os.environ else os.environ['AIROOT_DIR']\n",
        "os.chdir(airoot_dir)  #/content\n",
        "\n",
        "if _check_gpu:\n",
        "  check_gpu()\n",
        "\n",
        "start_time = time.time()\n",
        "!git clone -q --depth 1 -b {branchname} https://github.com/invoke-ai/InvokeAI/ IAekovnI\n",
        "print('项目克隆成功！')\n",
        "\n",
        "# @markdown ---\n",
        "os.chdir('IAekovnI')\n",
        "main_dir = os.environ['MAIN_DIR'] = os.getcwd() if 'MAIN_DIR' not in os.environ else os.environ['MAIN_DIR']\n",
        "db_dir = f'{main_dir}/db'\n",
        "print(\"正在安装环境,大约需要十分钟...\")\n",
        "\n",
        "def build_ui(branchname):\n",
        "  with capture.capture_output() as cap:\n",
        "    !apt-get -qq -y update\n",
        "    !apt-get -qq install build-essential python3-opencv libopencv-dev\n",
        "    !apt-get -qq install -y aria2 git #triton==2.0.0\n",
        "  if branchname == \"v3.5.1\" or branchname == \"v3.4.0post2\" or branchname == \"v3.3.0post3\":\n",
        "    print(\"当前使用的版本为：\", branchname)\n",
        "  else:\n",
        "    with capture.capture_output() as cap:\n",
        "      !curl -SLO https://deb.nodesource.com/nsolid_setup_deb.sh -o {main_dir}/nsolid_setup_deb.sh\n",
        "      !sudo chmod 500 {main_dir}/nsolid_setup_deb.sh\n",
        "      !{main_dir}/nsolid_setup_deb.sh 18\n",
        "      !apt-get -qq install nodejs -y\n",
        "      !apt-get -qq install npm -y\n",
        "      !npm install -g pnpm\n",
        "      !rm {main_dir}/nsolid_setup_deb.sh\n",
        "\n",
        "def build_web(branchname):\n",
        "  if branchname == \"v3.5.1\" or branchname == \"v3.4.0post2\" or branchname == \"v3.3.0post3\":\n",
        "    print(\"当前使用的版本为：\", branchname)\n",
        "  else:\n",
        "    with capture.capture_output() as cap:\n",
        "      %cd {main_dir}/invokeai/frontend/web\n",
        "      !pnpm i --frozen-lockfile\n",
        "      !pnpm build\n",
        "    print(\"新版前端UI构建完成!\")\n",
        "\n",
        "p = multiprocessing.Process(target=build_ui, args=(branchname,))\n",
        "p.start()\n",
        "\n",
        "#安装环境\n",
        "with capture.capture_output() as cap:\n",
        "    !pip install -q pyngrok pycloudflared -U\n",
        "    !pip install -q -e .  #用于构建Invoke\n",
        "    !invokeai-configure --root {db_dir} --yes --skip-sd-weights\n",
        "    build_web(branchname)\n",
        "\n",
        "%cd {main_dir}\n",
        "!wget -q https://huggingface.co/Vanwise/sd-colab/resolve/main/libtcmalloc_minimal.so.4\n",
        "os.environ['LD_PRELOAD'] = f'{main_dir}/libtcmalloc_minimal.so.4'\n",
        "!pip cache purge\n",
        "p.join()\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"环境安装完成，耗时：{:.3f}分钟.\".format((end_time - start_time) / 60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r4G2iRhtQtEe"
      },
      "outputs": [],
      "source": [
        "# @title (🔆)MOD！\n",
        "# @markdown #### 💠用于下载主模型、Lora、Vae、Embedding.\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from google.colab import files, drive\n",
        "from concurrent import futures\n",
        "\n",
        "# @markdown #### 启用MOD:\n",
        "enable_mod = True #@param {type:\"boolean\"}\n",
        "# @markdown #### 模型下载位置:\n",
        "modown_dir = \"colab\" # @param [\"colab\", \"drive\"]\n",
        "if modown_dir == \"drive\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive 挂载成功！')\n",
        "  modelr_dir = \"/content/drive/MyDrive/sd_models\"\n",
        "else:\n",
        "  modelr_dir = f'{db_dir}/models'\n",
        "# @markdown ---\n",
        "# @markdown ##### [MOD链接](https://docs.google.com/spreadsheets/d/1KViheGKazodhffMkQ0l_yPjAb6Bh9L4Y/edit?usp=sharing&ouid=109544984975183143943&rtpof=true&sd=true):\n",
        "mod_link = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQTjy-CxYnMoV_4BYruMz6Sl78Rqidqq_fyyoD2qtMMT6tOc0O9u1bf1Bwx-ECpUA/pub?output=xlsx\"# @param {type:\"string\"}\n",
        "# @markdown ##### MOD路径:\n",
        "mod_path = \"/content/drive/MyDrive/wise.xlsx\"# @param {type:\"string\"}\n",
        "# @markdown ##### 代码逻辑:优先使用MOD链接,链接为空使用MOD路径,路径为空使用上传MOD。\n",
        "\n",
        "def checkmod_link(link):\n",
        "  response = requests.get(link)\n",
        "  if response.status_code == 200:\n",
        "      content = response.content\n",
        "      try:\n",
        "          df = pd.read_excel(BytesIO(content))\n",
        "          print(\"MOD读取成功!\")\n",
        "          return df\n",
        "          #print(df.head())\n",
        "      except Exception as e:\n",
        "          print(\"无法正确读取文件,请检查文件格式是否为'.xlsx' :\", str(e))\n",
        "  else:\n",
        "      print(\"无法获取文件,请检查链接是否正确!\")\n",
        "\n",
        "def checkmod_path(path):\n",
        "  try:\n",
        "    df = pd.read_excel(path)\n",
        "    print(\"MOD读取成功!\")\n",
        "    return df\n",
        "    #print(df.head())\n",
        "  except Exception as e:\n",
        "    print(\"无法正确读取文件,请检查文件格式是否为'.xlsx' :\", str(e))\n",
        "\n",
        "def checkmod_up():\n",
        "    while True:\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                print(\"取消上传,使用默认MOD!\")\n",
        "                mod = 'https://github.com/Van-wise/sd-colab/raw/main/wise.xlsx'\n",
        "                df = pd.read_excel(mod)\n",
        "                break\n",
        "            mod_file = os.path.join(os.getcwd(), list(uploaded.keys())[0])\n",
        "            df = pd.read_excel(mod_file)\n",
        "            print(\"MOD读取成功!\")\n",
        "            break  # Exit the loop if the file is successfully read\n",
        "\n",
        "        except (ValueError, Exception) as e:\n",
        "            os.remove(mod_file)\n",
        "            print(\"无法正确读取文件，请检查文件格式是否为'.xlsx':\", str(e))\n",
        "            print(\"请重新上传文件。\")\n",
        "    return df\n",
        "\n",
        "def download_mod(file_dLlink, save_dir, file_name, index):\n",
        "    try:\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M {file_dLlink} -d {save_dir} -o {file_name}\n",
        "        print(f\"第{index+1}行:{file_name}下载成功!\")\n",
        "    except Exception as e:\n",
        "        print(f\"第{index+1}行:{file_name}文件下载错误：\", e)\n",
        "\n",
        "from concurrent import futures\n",
        "\n",
        "def start_mod():\n",
        "    if mod_link:\n",
        "        df = checkmod_link(mod_link)\n",
        "    elif mod_path:\n",
        "        df = checkmod_path(mod_path)\n",
        "    else:\n",
        "        df = checkmod_up()\n",
        "    df = df.fillna(\"\")\n",
        "\n",
        "    with futures.ThreadPoolExecutor() as executor:\n",
        "        future_to_index = {}\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            file_switch, file_version, file_name, file_type, file_dLlink = row[[\"开关\", \"版本\", \"名称\", \"类型\", \"下载地址\"]]\n",
        "\n",
        "            if any(value == \"\" for value in [file_switch, file_version, file_name, file_type, file_dLlink]):\n",
        "                print(f\"跳过,第{index+1}行,因为:|{'|'.join([key for key, value in row.items() if value == ''])}|的内容不能为空!\")\n",
        "                continue\n",
        "\n",
        "            save_dir = os.path.join(modelr_dir, file_version, file_type)\n",
        "            file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "            if not os.path.exists(file_path) and file_switch == \"on\":\n",
        "                future = executor.submit(download_mod, file_dLlink, save_dir, file_name, index)\n",
        "                future_to_index[future] = index\n",
        "\n",
        "        for future in futures.as_completed(future_to_index):\n",
        "            index = future_to_index[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"第{index+1}行下载错误:\", e)\n",
        "    print(\"MOD运行完成...🎉\")\n",
        "\n",
        "if enable_mod:\n",
        "  !aria2c --version > /dev/null 2>&1 || apt-get -qq install -y aria2\n",
        "\n",
        "  start_mod()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJ-XMf5R1_m"
      },
      "source": [
        "### (🔅)ControlNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y6bcyjEn33ME"
      },
      "outputs": [],
      "source": [
        "# @title 💠ControlNet\n",
        "\n",
        "# @markdown ##### 代码逻辑:不使用留空,需要使用选择与主模型对应的版本。(sdxl可以选择使用t2i_adapter)\n",
        "# @markdown ##### 🎈可以在 \"sd15_controlnets\" 数组里删掉或者注释掉(#)不想下载的模型.\n",
        "modelr_dir = f'{db_dir}/models'\n",
        "_controlnet = \"sd15\" # @param [\"\", \"sd15\"]\n",
        "sd15_controlnets = [\n",
        "    \"control_v11p_sd15_canny\",\n",
        "    \"control_v11p_sd15_mlsd\",\n",
        "    \"control_v11p_sd15_openpose\",\n",
        "    \"control_v11p_sd15_inpaint\",\n",
        "    \"control_v11e_sd15_ip2p\",\n",
        "    \"control_v11e_sd15_shuffle\",\n",
        "    \"control_v11p_sd15_softedge\",\n",
        "    \"control_v11p_sd15_scribble\",\n",
        "    \"control_v11p_sd15_lineart\",\n",
        "    \"control_v11p_sd15_seg\",\n",
        "    \"control_v11p_sd15_normalbae\",\n",
        "    \"control_v11f1p_sd15_depth\",\n",
        "]\n",
        "\n",
        "def download_controlnet(controlnet, controlnet_dir, split15_dir):\n",
        "  try:\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {controlnet_dir} -o diffusion_pytorch_model.fp16.safetensors https://huggingface.co/lllyasviel/{controlnet}/resolve/main/diffusion_pytorch_model.fp16.safetensors\n",
        "    #!aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {controlnet_dir} -o diffusion_pytorch_model.safetensors https://huggingface.co/lllyasviel/{controlnet}/resolve/main/diffusion_pytorch_model.safetensors\n",
        "    #!aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {controlnet_dir} -o diffusion_pytorch_model.bin https://huggingface.co/lllyasviel/{controlnet}/resolve/main/diffusion_pytorch_model.bin\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {controlnet_dir} -o config.json https://huggingface.co/lllyasviel/{controlnet}/resolve/main/config.json\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {controlnet_dir} -o control_net_{split15_dir}.py https://huggingface.co/lllyasviel/{controlnet}/resolve/main/control_net_{split15_dir}.py\n",
        "    #print(controlnet, \",下载成功!\")\n",
        "  except Exception as e:\n",
        "    print(controlnet, \"文件下载错误：\", e)\n",
        "\n",
        "if _controlnet == \"sd15\":\n",
        "    sd15cn_dir = f\"{modelr_dir}/sd-1/controlnet\"\n",
        "\n",
        "    with futures.ThreadPoolExecutor() as executor:\n",
        "        future_to_controlnet = {}\n",
        "\n",
        "        for controlnet in sd15_controlnets:\n",
        "            split15_dir = controlnet.split(\"_sd15_\")[1]\n",
        "            controlnet_dir = f\"{sd15cn_dir}/{split15_dir}\"\n",
        "            future = executor.submit(download_controlnet, controlnet, controlnet_dir, split15_dir)\n",
        "            future_to_controlnet[future] = controlnet\n",
        "\n",
        "        for future in futures.as_completed(future_to_controlnet):\n",
        "            controlnet = future_to_controlnet[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                print(controlnet, \"下载成功!\")\n",
        "            except Exception as e:\n",
        "                print(controlnet, \"文件下载错误：\", e)\n",
        "    print(\"ControlNet全部下载完成...🎉\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7-mU34LL3BuE"
      },
      "outputs": [],
      "source": [
        "# @title (可选)t2i_adapter\n",
        "# @markdown #### 💠t2i_adapter类似与controlnet,用于图像控制.\n",
        "# @markdown ##### 代码逻辑:不使用留空,需要使用选择与主模型对应的版本。(可重复运行)\n",
        "# @markdown ##### 🎈可以在 \"t2iadapters\" 数组里删掉或者注释掉(#)不想下载的模型.\n",
        "import concurrent.futures\n",
        "modelr_dir = f'{db_dir}/models'\n",
        "t2i_adapter = \"\" # @param [\"\", \"sd15\", \"sdxl\"]\n",
        "\n",
        "sd15_t2iadapters = [\n",
        "    \"t2iadapter_canny_sd15v2\",\n",
        "    \"t2iadapter_depth_sd15v2\",\n",
        "    \"t2iadapter_sketch_sd15v2\",\n",
        "    \"t2iadapter_zoedepth_sd15v1\",\n",
        "    \"t2iadapter_openpose_sd14v1\",\n",
        "    \"t2iadapter_keypose_sd14v1\",\n",
        "    \"t2iadapter_seg_sd14v1\",\n",
        "    \"t2iadapter_color_sd14v1\",\n",
        "]\n",
        "\n",
        "sdxl_t2iadapters = [\n",
        "    \"t2i-adapter-sketch-sdxl-1.0\",\n",
        "    \"t2i-adapter-depth-zoe-sdxl-1.0\",\n",
        "    \"t2i-adapter-openpose-sdxl-1.0\",\n",
        "    \"t2i-adapter-depth-midas-sdxl-1.0\",\n",
        "    \"t2i-adapter-lineart-sdxl-1.0\",\n",
        "    \"t2i-adapter-canny-sdxl-1.0\",\n",
        "]\n",
        "\n",
        "if t2i_adapter == \"sd15\":\n",
        "    sd15t2i_dir = f\"{modelr_dir}/sd-1/t2i_adapter\"\n",
        "\n",
        "    def download_t2iadapter(t2iadapter):\n",
        "        t2iadapter_dir = f\"{sd15t2i_dir}/{t2iadapter}\"\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {t2iadapter_dir} -o diffusion_pytorch_model.bin https://huggingface.co/TencentARC/{t2iadapter}/resolve/main/diffusion_pytorch_model.bin\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {t2iadapter_dir} -o config.json https://huggingface.co/TencentARC/{t2iadapter}/resolve/main/config.json\n",
        "        print(t2iadapter,\"下载成功!\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for t2iadapter in sd15_t2iadapters:\n",
        "            futures.append(executor.submit(download_t2iadapter, t2iadapter))\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            future.result()\n",
        "\n",
        "    print(\"t2i_adapter下载完成...🎉\")\n",
        "\n",
        "elif t2i_adapter == \"sdxl\":\n",
        "    sdxlt2i_dir = f\"{modelr_dir}/sdxl/t2i_adapter\"\n",
        "\n",
        "    def download_t2iadapter(t2iadapter):\n",
        "        t2iadapter_dir = f\"{sdxlt2i_dir}/{t2iadapter}\"\n",
        "        #!aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {t2iadapter_dir} -o diffusion_pytorch_model.safetensors https://huggingface.co/TencentARC/{t2iadapter}/resolve/main/diffusion_pytorch_model.safetensors\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {t2iadapter_dir} -o diffusion_pytorch_model.fp16.safetensors https://huggingface.co/TencentARC/{t2iadapter}/resolve/main/diffusion_pytorch_model.fp16.safetensors\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {t2iadapter_dir} -o config.json https://huggingface.co/TencentARC/t2i-adapter-sketch-sdxl-1.0/resolve/main/config.json\n",
        "        print(t2iadapter,\"下载成功!\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for t2iadapter in sdxl_t2iadapters:\n",
        "            futures.append(executor.submit(download_t2iadapter, t2iadapter))\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            future.result()\n",
        "\n",
        "    print(\"t2i_adapter全部下载完成...🎉\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T9smlWV42zP1"
      },
      "outputs": [],
      "source": [
        "# @title (可选)ip_adapter\n",
        "# @markdown #### 💠ip_adapter类似与controlnet,用于图像控制.\n",
        "ip_adapter = \"\" # @param [\"\", \"sd15\", \"sdxl\"]\n",
        "# @markdown ##### 代码逻辑:不使用留空,需要使用选择与主模型对应的版本。(可重复运行)\n",
        "# @markdown ##### 🎈可以在 \"ipadapters\" 数组里删掉不想下载的模型.\n",
        "modelr_dir = f'{db_dir}/models'\n",
        "if ip_adapter == \"sd15\":\n",
        "    sd15ip_dir = f\"{modelr_dir}/sd-1/ip_adapter\"\n",
        "    ipencoder_dir = f\"{modelr_dir}/any/clip_vision/ip_adapter_sd_image_encoder\"\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipencoder_dir} -o model.safetensors https://huggingface.co/InvokeAI/ip_adapter_sd_image_encoder/resolve/main/model.safetensors\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipencoder_dir} -o config.json https://huggingface.co/InvokeAI/ip_adapter_sd_image_encoder/resolve/main/config.json\n",
        "\n",
        "    ipadapters = [\"ip_adapter_sd15\", \"ip_adapter_sd15_light\", \"ip_adapter_plus_sd15\", \"ip_adapter_plus_face_sd15\"]\n",
        "    for ipadapter in ipadapters:\n",
        "        ipadapter_dir = f\"{sd15ip_dir}/{ipadapter}\"\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipadapter_dir} -o ip_adapter.bin https://huggingface.co/InvokeAI/{ipadapter}/resolve/main/ip_adapter.bin\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipadapter_dir} -o image_encoder.txt https://huggingface.co/InvokeAI/{ipadapter}/resolve/main/image_encoder.txt\n",
        "        print(ipadapter,\"下载成功!\")\n",
        "elif ip_adapter == \"sdxl\":\n",
        "    sdxlip_dir = f\"{modelr_dir}/sdxl/ip_adapter\"\n",
        "    ipencoder_dir = f\"{modelr_dir}/any/clip_vision/ip_adapter_sdxl_image_encoder\"\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipencoder_dir} -o model.safetensors https://huggingface.co/InvokeAI/ip_adapter_sdxl_image_encoder/resolve/main/model.safetensors\n",
        "    !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipencoder_dir} -o config.json https://huggingface.co/InvokeAI/ip_adapter_sdxl_image_encoder/resolve/main/config.json\n",
        "\n",
        "    ipadapters = [\"ip_adapter_sdxl\", \"ip_adapter_sdxl_vit_h\", \"ip_adapter_plus_sdxl_vit_h\"]\n",
        "    for ipadapter in ipadapters:\n",
        "        ipadapter_dir = f\"{sdxlip_dir}/{ipadapter}\"\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipadapter_dir} -o ip_adapter.bin https://huggingface.co/InvokeAI/{ipadapter}/resolve/main/ip_adapter.bin\n",
        "        !aria2c --console-log-level=error -q -c -x 16 -s 16 -k 1M -d {ipadapter_dir} -o image_encoder.txt https://huggingface.co/InvokeAI/{ipadapter}/resolve/main/image_encoder.txt\n",
        "        print(ipadapter,\"下载成功!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7fE5R-iBrl0"
      },
      "source": [
        "### (⚒)附加功能：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Zek2dJ_pC5Hu"
      },
      "outputs": [],
      "source": [
        "# @markdown #### 📁打包输出图片:\n",
        "使用打包图片功能 = True # @param {type:\"boolean\"}\n",
        "输出图片到云盘 = False # @param {type:\"boolean\"}\n",
        "打包图片到云盘 = False # @param {type:\"boolean\"}\n",
        "#@markdown #####保存间隔（秒）：\n",
        "save_interval = 200 #@param {type:\"integer\"}\n",
        "\n",
        "if 输出图片到云盘:\n",
        "  outputs_path = \"/content/drive/MyDrive/sd_models/outputs\"\n",
        "else:\n",
        "  outputs_path = \"/content/IAekovnI/db/outputs\"\n",
        "\n",
        "if 打包图片到云盘:\n",
        "  zip_path = \"/content/drive/MyDrive/sd_models/\"\n",
        "else:\n",
        "  zip_path = \"/content/outputs\"\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def zip_outputs(outputs_path, zip_path, save_interval):\n",
        "  save_count = 0\n",
        "  last_backup_path = None\n",
        "\n",
        "  while True:\n",
        "    time.sleep(save_interval)\n",
        "\n",
        "    save_count += 1\n",
        "    backup_name = f\"{datetime.now().strftime('%m_%d')}_outputs_{datetime.now().strftime('%H:%M')}_{save_count}\"\n",
        "    backup_path = f\"{zip_path}/{backup_name}\"\n",
        "\n",
        "    if last_backup_path:\n",
        "      os.remove(last_backup_path)\n",
        "\n",
        "    try:\n",
        "      shutil.make_archive(backup_path, 'zip', outputs_path)\n",
        "      print(f'Backup created {backup_path}.zip')\n",
        "      last_backup_path = f\"{backup_path}.zip\"\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred while creating the backup: {e}\")\n",
        "\n",
        "import multiprocessing\n",
        "def start_background_task(target_func, *args):\n",
        "  p = multiprocessing.Process(target=target_func, args=args)\n",
        "  p.start()\n",
        "  return p\n",
        "\n",
        "def stop_background_task(process):\n",
        "  process.terminate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAQ8GzZFHr7X"
      },
      "source": [
        "### 启动窗口"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "Iavb_grm888s"
      },
      "outputs": [],
      "source": [
        "# @title 🛜内网穿透\n",
        "\n",
        "import os\n",
        "import shlex\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "# @markdown ##### 启动语音:\n",
        "use_audio = False # @param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "# @markdown #### [Ngrok](https://dashboard.ngrok.com/get-started/your-authtoken):\n",
        "ngrok_token = \"\" # @param {type:\"string\"}\n",
        "# @markdown #### cloudflare:\n",
        "use_cloudflare = True # @param {type:\"boolean\"}\n",
        "# @markdown #### remote:\n",
        "use_remote = True # @param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "# @markdown #### 端口设置：\n",
        "iport = \"9090\" # @param [\"9090\", \"8080\", \"7680\"] {allow-input: true}\n",
        "\n",
        "def gen_key(path: Union[str, Path]) -> None:\n",
        "    path = Path(path)\n",
        "    arg_string = f'ssh-keygen -t rsa -b 4096 -N \"\" -q -f {path.as_posix()}'\n",
        "    args = shlex.split(arg_string)\n",
        "    subprocess.run(args, check=True)\n",
        "    path.chmod(0o600)\n",
        "\n",
        "if use_remote:\n",
        "  id_rsa_file = f\"{airoot_dir}/id_rsa\"\n",
        "  id_rsa_pub_file = f\"{airoot_dir}/id_rsa.pub\"\n",
        "  if os.path.exists(id_rsa_file):\n",
        "      os.remove(id_rsa_file)\n",
        "  if os.path.exists(id_rsa_pub_file):\n",
        "      os.remove(id_rsa_pub_file)\n",
        "  ssh_name = \"id_rsa\"\n",
        "  ssh_path = Path(os.path.dirname(os.getcwd())) / ssh_name\n",
        "  gen_key(ssh_path)\n",
        "\n",
        "if ngrok_token:\n",
        "  from pyngrok import ngrok\n",
        "  ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "def start_tunnle():\n",
        "\n",
        "  open(\"/content/output.log\", \"w\").close()\n",
        "  # 循环检测文件内容，直到存在\"http://127.0.0.1:9090\"。\n",
        "  while not f\"http://127.0.0.1:{iport}\" in open(\"/content/output.log\", \"r\").read():\n",
        "    time.sleep(1)\n",
        "\n",
        "  clear_output()\n",
        "  if use_audio:\n",
        "    from IPython.display import display, HTML\n",
        "    audio_url = \"https://github.com/Van-wise/sd-colab/raw/main/qidong.mp3\"\n",
        "    display(HTML(f'<audio src=\"{audio_url}\" controls autoplay style=\"display:none\"></audio>'))\n",
        "\n",
        "  if ngrok_token:\n",
        "      try:\n",
        "          from pyngrok import ngrok\n",
        "          ngrok_tunnel = ngrok.connect(iport, \"http\")\n",
        "          print(\"ngrok_tunnel:\", ngrok_tunnel)\n",
        "      except Exception as e:\n",
        "          print(\"ngrok 连接失败：\", e)\n",
        "\n",
        "  if use_cloudflare:\n",
        "      try:\n",
        "          from pycloudflared import try_cloudflare\n",
        "          cloudflare_url = try_cloudflare(iport, verbose=False)\n",
        "          print(\"cloudflare_tunnel:\", cloudflare_url)\n",
        "      except Exception as e:\n",
        "          print(\"cloudflare 连接失败：\", e)\n",
        "\n",
        "  if use_remote:\n",
        "      !ssh -R 80:127.0.0.1:{iport} -o StrictHostKeyChecking=no -i {airoot_dir}/id_rsa remote.moe 2>&1 | tee -a /content/tunnel.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Ag8DsiS8-GJ"
      },
      "outputs": [],
      "source": [
        "# @title 🎨启动\n",
        "import threading\n",
        "import sys\n",
        "import time\n",
        "\n",
        "if '使用打包图片功能' in globals():\n",
        "    if 使用打包图片功能:\n",
        "        outputs_paths = f\"{outputs_path}/images\"\n",
        "        back_zip = start_background_task(zip_outputs, outputs_paths, zip_path, save_interval)\n",
        "else:\n",
        "    pass\n",
        "\n",
        "if 'outputs_path' in globals():\n",
        "  pass\n",
        "else:\n",
        "  outputs_path = f'{main_dir}/db/outputs'\n",
        "\n",
        "threading.Thread(target=start_tunnle, daemon=True).start()\n",
        "\n",
        "!echo \"3\" | invokeai-web --root {main_dir}/db --ignore_missing_core_models --port {iport} --outdir $outputs_path 2>&1 | tee /content/output.log\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    time.sleep(60)\n",
        "    print(\"链接可能出想错误...\")\n",
        "    print(\"请先关闭线程、修改端口再次尝试!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfY3OoJczidS"
      },
      "outputs": [],
      "source": [
        "# @title 💽备份\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def sync_data():\n",
        "  !cp -r -f /content/IAekovnI/db/invokeai.yaml /content/drive/MyDrive/sd_models/invokeai.yaml\n",
        "  !cp -r -f /content/IAekovnI/db/databases/invokeai.db /content/drive/MyDrive/sd_models/invokeai.db\n",
        "  !cp -r -f /content/IAekovnI/db/configs/models.yaml /content/drive/MyDrive/sd_models/models.yaml\n",
        "\n",
        "def sync_outputs():\n",
        "  !mkdir -p /content/drive/MyDrive/sd_models/pic\n",
        "  !cp -r -f /content/IAekovnI/db/outputs/* /content/drive/MyDrive/sd_models/outputs\n",
        "\n",
        "sync_data()\n",
        "sync_outputs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eocexA6zs83I"
      },
      "source": [
        "### 关闭进程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtKvGymh6b-w",
        "outputId": "d63d3c95-cb04-47a5-9ba7-2346681c5142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "进程已关闭!\n"
          ]
        }
      ],
      "source": [
        "!pkill -f pyngrok\n",
        "!pkill -f cloudflared\n",
        "!pkill -f invokeai\n",
        "!pkill -f ssh\n",
        "stop_background_task(back_zip)\n",
        "print(\"进程已关闭!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HQjrBnpQrdnZ",
        "eocexA6zs83I"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
